# Chat Logger For Local LLM 
If you are using a GUI for ollama this does not apply since the GUI automatically saves chats. 
By default ollama does not save your chat logs if you decide to run directly on your terminal.
This is where we use a python script to run ollama, as well as save the transcript for your chat directly to your file system. 
1. Run: pip install llama 
2. Run: ollama pull llama3
3. Open VS code. 
4. Create a folder to store VSCODE files 
5. Create a file e.g. Chat_logger.py paste the following script
6. Below
''' 


'''
